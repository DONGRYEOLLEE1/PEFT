{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA\n",
    "\n",
    "LoRA는 Lora-Rank Decomposition 방법을 사용하여, 기존의 모델 파라미터를 직접 업데이트하지 않고, **차이를 보정하는 작은 크기의 두 행렬**을 학습합니다. \n",
    "\n",
    "- 기존 모델 파라미터 고정\n",
    "    - LoRA는 기존의 큰 파라미터 행렬을 freeze하고, 새로운 파라미터인 A와 B만 학습합니다. LoRA는 모델의 기존 파라미터를 재활용하면서, 필요한 적은 수의 추가 파라미터만 학습하는 방식입니다.\n",
    "- 저차원 근사화\n",
    "    - LoRA는 **저차원 행렬(Low-Rank Matrix)**을 사용하여 파라미터를 근사화합니다. 예를 들어, W = BA 형태로 분해하여, 고차원의 가중치 행렬을 두 개의 작은 행렬로 분해하고, 이 두 작은 행렬만 학습합니다. 여기서 A와 B는 low-rank 행렬이기 때문에, 고차원 행렬을 사용하는 것보다 훨씬 적은 메모리와 계산량을 소모하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 가능한 파라미터 수 계산 함수\n",
    "def count_trainable_params(model):\n",
    "    return format(sum(p.numel() for p in model.parameters() if p.requires_grad), \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model\n",
    "class SimpleLM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "vocab_size = 10000\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 가능한 파라미터 수 (기본 모델): 10,512,656\n"
     ]
    }
   ],
   "source": [
    "model = SimpleLM(vocab_size, hidden_size)\n",
    "\n",
    "print(f\"학습 가능한 파라미터 수 (기본 모델): {count_trainable_params(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLM(\n",
       "  (embed): Embedding(10000, 512)\n",
       "  (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Model\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r = 4, alpha = 1.0):\n",
    "        \"\"\"\n",
    "        in_features: 입력차원(d)\n",
    "        out_features: 출력차원(d)\n",
    "        r: LoRA에서 사용하는 low-rank 행렬의 rank값 (논문에선 4~8 값 사용)\n",
    "        alpha: scaling factor (논문에선 rank값과 함께 LoRA의 학습 안정성을 위해 사용)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        # low-rank 행렬의 영향을 조절\n",
    "        # scaling-factor의 수학적 역할\n",
    "        # 모델의 기존 가중치와 변경된 가중치간의 비율을 적절히 맞추기 위함\n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        # 기존 weight freeze\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.weight.requires_grad = False\n",
    "        \n",
    "        # 새로 학습할 A,B행렬 정의\n",
    "        # ΔW = B @ A\n",
    "        # A: (r x in_features), 초기값은 작은 값으로 초기화 (논문에선 0에 가까운 초기화 권장)\n",
    "        self.A = nn.Parameter(torch.randn(r, in_features) * 0.01)\n",
    "        # B: (out_features x r)\n",
    "        self.B = nn.Parameter(torch.randn(out_features, r) * 0.01)\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        기존 W_o는 고정하고, ΔW = B @ A를 통해 weight를 보정\n",
    "        최종적으로 W' = W_o + ΔW (scaling factor 곱함)\n",
    "        \n",
    "        f(x) = (W_o + ΔW) @ x + b\n",
    "             = (W_o + scaling-factor * (B @ A)) @ x + b\n",
    "        \"\"\"\n",
    "        lora_weight = self.weight + (self.B @ self.A) * self.scaling\n",
    "        # 기존의 linear operation\n",
    "        return nn.functional.linear(x , lora_weight, self.bias)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        \"\"\"torch.nn.Module에서 __repr__의 상세정보를 덧붙일 때 사용하는 전용 메서드\"\"\"\n",
    "        return f\"in_features = {self.in_features}, out_features = {self.out_features}, r = {self.r}, alpha = {self.alpha}\"\n",
    "    \n",
    "class SimpleLMWithLoRA(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, r = 4, alpha = 1.0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        self.lora_linear1 = LoRALinear(hidden_size, hidden_size, r, alpha)\n",
    "        self.lora_linear2 = LoRALinear(hidden_size, vocab_size, r, alpha)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear2(torch.relu(self.linear1(self.embed(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 가능한 파라미터 수 (LoRA 모델델): 56,656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLinear1\\n    - A: (r x in_features)  = (4 x 512)     = 2048\\n    - B: (out_features x r) = (512 x 4)     = 2048\\n    - bias: (out_features)                  = 512\\n\\nLinear2:\\n    - A: (r x in_features)  = (4 x 512)     = 2048\\n    - B: (out_features x r) = (10000 x 4)   = 40000\\n    - bias: (out_featrues)                  = 10000\\n\\nTrainable Parameters: {Linear1} (4 x 512) + (512 x 4) + 512 +                       {Linear2} (4 x 512) + (10000 x 4) + 1000\\n                      = 56,656\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model = SimpleLMWithLoRA(vocab_size, hidden_size, r = 4, alpha = 16)\n",
    "\n",
    "print(f\"학습 가능한 파라미터 수 (LoRA 모델델): {count_trainable_params(lora_model)}\")\n",
    "\n",
    "\"\"\"\n",
    "Linear1\n",
    "    - A: (r x in_features)  = (4 x 512)     = 2048\n",
    "    - B: (out_features x r) = (512 x 4)     = 2048\n",
    "    - bias: (out_features)                  = 512\n",
    "    \n",
    "Linear2:\n",
    "    - A: (r x in_features)  = (4 x 512)     = 2048\n",
    "    - B: (out_features x r) = (10000 x 4)   = 40000\n",
    "    - bias: (out_featrues)                  = 10000\n",
    "    \n",
    "Trainable Parameters: {Linear1} (4 x 512) + (512 x 4) + 512 + \\\n",
    "                      {Linear2} (4 x 512) + (10000 x 4) + 1000\n",
    "                      = 56,656\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLMWithLoRA(\n",
       "  (embed): Embedding(10000, 512)\n",
       "  (lora_linear1): LoRALinear(in_features = 512, out_features = 512, r = 4, alpha = 16)\n",
       "  (lora_linear2): LoRALinear(in_features = 512, out_features = 10000, r = 4, alpha = 16)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peft - LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import get_peft_model\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=16, target_modules={'q_proj', 'embed_tokens', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'331,196,416'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(sum(p.numel() for p in model.parameters() if p.requires_grad), \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\first-uv\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.decoder.embed_tokens'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): lora.Embedding(\n",
       "            (base_layer): Embedding(50272, 512, padding_idx=1)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict()\n",
       "            (lora_B): ModuleDict()\n",
       "            (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x50272])\n",
       "            (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 512x16])\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,385,408 || all params: 333,581,824 || trainable%: 0.7151\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): lora.Embedding(\n",
       "        (base_layer): Embedding(50272, 512, padding_idx=1)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict()\n",
       "        (lora_B): ModuleDict()\n",
       "        (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x50272])\n",
       "        (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 512x16])\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "unloaded_model = lora_model.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(model) == id(unloaded_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
